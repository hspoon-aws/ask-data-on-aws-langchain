{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 101\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "- GitHub: https://github.com/hwchase17/langchain\n",
    "- Docs: https://python.langchain.com/en/latest/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLMs\n",
    "\n",
    "A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def init_llm_openai():\n",
    "    # os.environ[\"OPENAI_API_KEY\"] =\"YOUR_OPENAI_TOKEN\"\n",
    "\n",
    "    llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
    "    return llm\n",
    "   \n",
    "llm = init_llm_openai()\n",
    "\n",
    "text = \"What would be a good AWS new service name that allow customers to chat with their own data\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SagaMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "def init_llm_sm_endpoint():\n",
    "\n",
    "    endpoint_name = 'jumpstart-dft-meta-textgeneration-llama-2-13b-f'\n",
    "    aws_region='us-east-1'\n",
    "    parameters = {\"max_new_tokens\": 700, \"temperature\": 0.1}\n",
    "\n",
    "\n",
    "    class ContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        # def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        #     input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        #     return input_str.encode(\"utf-8\")\n",
    "        def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "            input_str = json.dumps({\"inputs\" : [[{\"role\" : \"system\",\n",
    "            \"content\" : \"You are a kind robot.\"},\n",
    "            {\"role\" : \"user\", \"content\" : prompt}]],\n",
    "            \"parameters\" : {**model_kwargs}})\n",
    "            return input_str.encode('utf-8')\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            return response_json[0][\"generation\"][\"content\"]\n",
    "\n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    sm_llm = SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_handler,\n",
    "        endpoint_kwargs={\"CustomAttributes\": \"accept_eula=true\"},\n",
    "    )\n",
    "    return sm_llm\n",
    "\n",
    "llm = init_llm_sm_endpoint()\n",
    "text = \"What would be a good AWS new service name that allow customers to chat with their own data? Keep answer short. Just give me one answer in your reponse\"\n",
    "llm(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates\n",
    "\n",
    "LangChain faciliates prompt management and optimization.\n",
    "\n",
    "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Question: What would be a good AWS new service name that allow customers to chat with their own data?\n",
    "\n",
    "Give a marketing slogan for the service.\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: What would be a good AWS new service name that {feature}.\n",
    "\n",
    "Let's think step by step. Just answer the designed name\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = prompt.format(feature=\"allow customers to chat with their own data\")\n",
    "print(f\"Prompt = \\n {input}\")\n",
    "\n",
    "print(llm(input))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chains\n",
    "\n",
    "Combine LLMs and Prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: What would be a good AWS new service name that {feature}.\n",
    "\n",
    "Let's think step by step. Just answer the designed name\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "name_creation_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "feature = \"Chat with data on AWS\"\n",
    "print(name_creation_chain.run(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an LLMChain to write a marketing slogan.\n",
    "\n",
    "template = \"\"\"You are a marketing agency. Given a AWS service name, it is your job to write a marketing slogan for that service.\n",
    "Let's think step by step. \n",
    "\n",
    "Service: {name}\n",
    "Slogan: This is a marketing slogan for the above service:\n",
    "Explanation: This is the explanation of the slogan and why you think it is good:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"name\"], template=template)\n",
    "slogan_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "slogan_chain.run(\"AWS Data Chatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[name_creation_chain, slogan_chain], verbose=True)\n",
    "\n",
    "new_launch = overall_chain.run(\"Chat with your data on AWS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agents and Tools\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "\n",
    "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "- LLM: The language model powering the agent.\n",
    "- Agent: The agent to use.\n",
    "\n",
    "Tools: https://python.langchain.com/en/latest/modules/agents/tools.html\n",
    "\n",
    "Agent Types: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqq wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"In what year was the Amazon S3 released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Chat with your data on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -qqq install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Demo 1]: Read data on S3 and Talk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RAG\n",
    "- Load Document (PDF, docx, text) from S3 \n",
    "- Store into Vector store \n",
    "- QnA using LLM with RetrievalQA chain provided by LangChain\n",
    "\n",
    "Ref: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read a PDF document from S3 using S3FileLoader\n",
    "from langchain.document_loaders import S3FileLoader\n",
    "loader = S3FileLoader(\"BUCKET\", \"FILE\")\n",
    "all_splits = loader.load_and_split()\n",
    "print(f\"Original: Number of document splits = {len(all_splits)}\")\n",
    "\n",
    "\n",
    "# Embedding and Store into Vector Store\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "## TODO: use AOS as vector store\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "service = 'es' # must set the service as 'aoss'\n",
    "region = 'us-east-1'\n",
    "# credentials = boto3.Session(aws_access_key_id='xxxxxx',aws_secret_access_key='xxxxx').get_credentials()\n",
    "# awsauth = AWS4Auth('xxxxx', 'xxxxxx', region,service, session_token=credentials.token)\n",
    "\n",
    "vectorstore = OpenSearchVectorSearch.from_documents(\n",
    "    all_splits,\n",
    "    OpenAIEmbeddings(),\n",
    "    opensearch_url=\"https://xxxxxx.us-east-1.es.amazonaws.com\",\n",
    "    http_auth=(\"admin\", \"password\"),\n",
    "    timeout = 300,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=\"test-index-using-aoss\",\n",
    "    engine=\"faiss\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Why need a Modern Data architecture? summarize in 100 words\"\n",
    "# question = \"What are the pillars of a Modern Data architecture? summarize in 100 words\"\n",
    "question = \"Explain about Modern Data Architecture with a 5 year-old kid. summarize in 100 words\"\n",
    "docs = vectorstore.similarity_search(question, k=10)\n",
    "print(f\"Vector search: Number of document related to the question = {len(docs)}\")\n",
    "\n",
    "# QnA the content using RetrievalQA chain provided by Langchain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "output = qa_chain({\"query\": question})\n",
    "print(output['result'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read CSV from S3 into Pandas. Chat with LLM using Pandas DataFrame Agent provided by LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV from S3 into Pandas\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "df = pd.read_csv(\"s3://BUCKET/titanic.csv\") # install s3fs\n",
    "\n",
    "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), df, verbose=True)\n",
    "\n",
    "agent.run(\"Give the statistic on survived distribution by gender\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Demo 2]: Chat with your data on AWS by Text-to-SQL\n",
    "\n",
    "In LangChain, there are SQLDatabaseChain and SQL Agents for Text-to-SQL query and execution.\n",
    "\n",
    "Ref: https://python.langchain.com/docs/use_cases/sql\n",
    "\n",
    "It is very good for popular data sources supported by SQLAlchemy (such as MySQL, PostgreSQL, Oracle SQL, Databricks, SQLite). \n",
    "\n",
    "However, the **pre-defined prompts inside the tools are restricted and may not optimized for some SQL engine (e.g. Amazon Athena)**. \n",
    "Therefore, I customize the Prompt template into the LangChain SQLDatabaseChain for better result.\n",
    "\n",
    "Instead of using pre-built agent, here i use \n",
    "1. `create_sql_query_chain` module to perform text-to-sql using LLM\n",
    "2. execute the SQL script by pushing down the SQL into Athena using Pandas.\n",
    "3. Display the DataFrame into UI instead of summarizing into human-readable reponse. (Because some customers may just want to it as data extraction without writing SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define athena connection engine\n",
    "from  sqlalchemy import create_engine\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "region = 'us-east-1'\n",
    "glue_database_name='chinook'\n",
    "glue_databucket_name='aws-athena-query-results-xxxxxxxx-us-east-1'\n",
    "\n",
    "connathena=f\"athena.{region}.amazonaws.com\" \n",
    "portathena='443' #Update, if port is different\n",
    "schemaathena=glue_database_name #from cfn params\n",
    "s3stagingathena=f's3://{glue_databucket_name}/athenaresults/'#from cfn params\n",
    "wkgrpathena='primary'#Update, if workgroup is different\n",
    "\n",
    "##  Create the athena connection string\n",
    "connection_string = f\"awsathena+rest://@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}/&work_group={wkgrpathena}\"\n",
    "##  Create the athena  SQLAlchemy engine\n",
    "engine_athena = create_engine(connection_string, echo=False)\n",
    "\n",
    "db = SQLDatabase(engine_athena, sample_rows_in_table_info=0, custom_table_info={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize prompt for Athena\n",
    "## - use Presto SQL syntax\n",
    "## - Make sure selecting the columns only which is in GROUP BY.\n",
    "## - If you use string indicating a date, add date before the string. For example, date '2012-01-01'.\n",
    "## - Rename the columns to the best of answering the question.\n",
    "## - If you think the question is not related to any tables in the database, just reply 'Sorry, it seems not related to the data'.\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "PROMPT_SUFFIX = \"\"\"Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct Presto query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "\n",
    "Make sure selecting the columns only which is in GROUP BY. If the group column is a aggregated column, make sure the same aggregation is used in GROUP BY.\n",
    "\n",
    "If you use string indicating a date, add date before the string. For example, date '2012-01-01'. Other than this, avoid to use date and time functions and Operator which may not be supported in Presto query.\n",
    "\n",
    "Rename the columns to the best of answering the question.\n",
    "\n",
    "Review the answer and improve before giving the answer.\n",
    "\n",
    "If you think the question is not related to any tables in the database, just reply 'Sorry, it seems not related to the data'.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\",  \"top_k\"],\n",
    "    template=_DEFAULT_TEMPLATE + PROMPT_SUFFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = create_sql_query_chain(llm, db, prompt=CUSTOM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "# question = \"Find top 5 best selling albums. Add corresponding artist, total revenue, total duration in minutes as columns\"\n",
    "question = 'monthly sales volume and revenue trend over time'\n",
    "sql_response = chain.invoke({\"question\":question})\n",
    "\n",
    "print(sql_response)\n",
    "\n",
    "sql_keywords_regex = r'^(SELECT|WITH|INSERT|UPDATE|DELETE|CREATE|DROP|ALTER)'\n",
    "match = re.search(sql_keywords_regex, sql_response.strip(), re.IGNORECASE)\n",
    "if match:\n",
    "    with engine_athena.connect() as conn:\n",
    "        df = pd.read_sql_query(text(sql_response), con=conn)\n",
    "        print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a better UI\n",
    "! streamlit run st-demo-gen-sql.py > /dev/null 2>&1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo : visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.agents import (AgentExecutor, Tool, ZeroShotAgent,\n",
    "                              initialize_agent, load_tools)\n",
    "from langchain_experimental.sql import SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    verbose=True,  # Show its work\n",
    "    return_direct=True,  # Return the results without sending back to the LLM\n",
    "    prompt=CUSTOM_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add python_repl to our list of tools\n",
    "tools = load_tools([\"python_repl\"])\n",
    "\n",
    "# Define our voter_data tool\n",
    "\n",
    "# Set a description to help the LLM know when and how to use it.\n",
    "description = (\n",
    "    \"Useful for when you need to answer questions about chinook data. \"\n",
    "    # \"You must not input SQL. Use this more than the Python tool if the question \"\n",
    "    # \"is about chinook data, such as albums, artists, invoice, playlist, track.\"\n",
    ")\n",
    "\n",
    "chinook_data = Tool(\n",
    "    name=\"AthenaQuery\",  # We'll just call it 'Data'\n",
    "    func=db_chain.run,\n",
    "    description=description\n",
    ")\n",
    "\n",
    "tools.append(chinook_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard prefix\n",
    "prefix = \"Fulfill the following request as best you can. You have access to the following tools:\"\n",
    "\n",
    "# Remind the agent of the Data tool, and what types of input it expects\n",
    "suffix = (\n",
    "    \"Begin! When looking for data, do not write a SQL query. \"\n",
    "    \"Pass the relevant portion of the request directly to the Data tool in its entirety.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Request: {input}\\n\"\n",
    "    \"{agent_scratchpad}\"\n",
    ")\n",
    "\n",
    "# The agent's prompt is built with the list of tools, prefix, suffix, and input variables\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n",
    ")\n",
    "\n",
    "# Set up the llm_chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Specify the tools the agent may use\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n",
    "\n",
    "# Create the AgentExecutor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "request = \"Show a bar graph visualizing the answer to the following question:\" \\\n",
    "        \"Find top 3 best selling employees in terms of revenue\"\n",
    "\n",
    "agent_executor.run(request)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Show a line graph visualizing the answer to the following question:\" \\\n",
    "        \"show me the sales volume and revenue trend over time for recent 50 sales invoice\"\n",
    "\n",
    "agent_executor.run(request)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Demo 3] - AWS SDK for pandas (awswrangler) LangChain Tool \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = '''You are a python programming expert who. Your job is to write a python code using AWS SDK for pandas (awswrangler) to complete below tasks as your best.\n",
    "Let's think step by step. \n",
    "\n",
    "Use below output format:\n",
    "Task: {task}\n",
    "Python code:'''\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"task\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = coder.run('list all index from opensearch')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = coder.run('Create EMR Cluster. then group all small file in s3://data/*.json into a big file using the EMR')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OpenAI()(\"what is the latest version of AWS SDK for pandas (awswrangler) you know \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OpenAI()(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab url from the api doc\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://aws-sdk-pandas.readthedocs.io/en/stable/api.html\"\n",
    "r=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(r.content,\"html.parser\")\n",
    "a_href=soup.find_all(\"a\",{\"class\":\"reference internal\"}, href=True)\n",
    "print(len(a_href))\n",
    "\n",
    "urls = [url]\n",
    "for a in a_href:\n",
    "    if(a['href'].startswith('stub')):\n",
    "        urls.append(f\"https://aws-sdk-pandas.readthedocs.io/en/stable/{a['href']}\")\n",
    "\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "\n",
    "# urls = [\"https://aws-sdk-pandas.readthedocs.io/en/stable/api.html\"]\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "html_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.HTML\n",
    ")\n",
    "all_splits = loader.load_and_split(text_splitter = html_splitter)\n",
    "# all_splits = loader.load_and_split()\n",
    "\n",
    "print(f\"Original: Number of document = {len(all_splits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding and Store into Vector Store\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "service = 'es' # must set the service as 'aoss'\n",
    "region = 'us-east-1'\n",
    "# credentials = boto3.Session(aws_access_key_id='xxxxxx',aws_secret_access_key='xxxxx').get_credentials()\n",
    "# awsauth = AWS4Auth('xxxxx', 'xxxxxx', region,service, session_token=credentials.token)\n",
    "\n",
    "vectorstore = OpenSearchVectorSearch.from_documents(\n",
    "    all_splits,\n",
    "    OpenAIEmbeddings(),\n",
    "    opensearch_url=\"https://search-vectorstore-xxxxxxxxx.us-east-1.es.amazonaws.com\",\n",
    "    http_auth=(\"admin\", \"Login123!\"),\n",
    "    timeout = 300,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=\"awswrangler-api-index\",\n",
    "    engine=\"faiss\",\n",
    "    bulk_size=5000\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Using awswrangler, Execute Clean Rooms Protected SQL query and return the results as a Pandas DataFrame.'\n",
    "docs = vectorstore.similarity_search(question, k=10)\n",
    "print(f\"Vector search: Number of document related to the question = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QnA the content using RetrievalQA chain provided by Langchain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the latest version of AWS SDK for pandas (awswrangler) you know?\"\n",
    "before_rag = llm.predict(query)\n",
    "print(f\"Answer before RAG : {before_rag}\")\n",
    "\n",
    "print(\"\\n================\\n\")\n",
    "output = qa_chain({\"query\":query})\n",
    "\n",
    "print(f\"Answer After rag : {output['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Using awswrangler, Execute Clean Rooms Protected SQL query and return the results as a Pandas DataFrame.\"\n",
    "before_rag = llm.predict(query)\n",
    "print(f\"Answer before RAG : {before_rag}\")\n",
    "\n",
    "print(\"\\n================\\n\")\n",
    "output = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"Answer After rag : {output['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code generate chain: 1) Gen code 2) code review 3) output improved code\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "step1_code_plan_template = '''You are a python programming expert. Your job is to plan step by step how to write a python code using AWS SDK for pandas (awswrangler) to complete below tasks at your best.\n",
    "Use the following pieces of context related to awswrangler api to complete the task. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "\n",
    "\n",
    "Use the following output format:\n",
    "Task: <The task to be implemented>\n",
    "Plan:\n",
    "1. <the first step that have to be done when writing a python code. list all possible awswrangler APIs if needed>\n",
    "2. <the second step that have to be done when writing a python code. list all possible awswrangler APIs if needed>\n",
    "3. <the third step that have to be done when writing apython code. list all possible awswrangler APIs if needed>\n",
    "(You can plan up to 8 steps)\n",
    "\n",
    "Let's think step by step. \n",
    "\n",
    "Double check if all the awswrangler APIs exist in the context.\n",
    "\n",
    "Begin!\n",
    "Task: {question}\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "step1_code_plan_prompt = PromptTemplate(template=step1_code_plan_template, input_variables=[\"context\", \"question\"])\n",
    "# chain_type_kwargs = {\"prompt\": step1_code_plan_prompt}\n",
    "# step1_gen_code_chain = LLMChain(prompt=step1_gen_code, llm=llm)\n",
    "step1_code_plan_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "\n",
    "step2_code_gen_template = '''You are a senior python programmer. Your job is to write python code using AWS SDK for pandas (awswrangler) to complete the planning at your best.\n",
    "Let's think step by step. \n",
    "\n",
    "Use the following output format:\n",
    "``` python\n",
    "## Step1: <the first step in the plan>\n",
    "<code to implement step 1>\n",
    "\n",
    "## Step2: <the second step in the plan>\n",
    "<code to implement step 2>\n",
    "\n",
    "## Step3: <the third step in the plane>\n",
    "<code to implement step 3>\n",
    "\n",
    "```\n",
    "\n",
    "Begin!\n",
    "Plan: {step1}\n",
    "'''\n",
    "step2_code_gen_prompt= PromptTemplate(template=step2_code_gen_template, input_variables=[\"step1\"])\n",
    "# chain_type_kwargs = {\"prompt\": step2_code_review_prompt}\n",
    "# step2_code_review_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "step2_code_gen_chain = LLMChain(prompt=step2_code_gen_prompt, llm=llm)\n",
    "\n",
    "step2_code_review_template = '''You are a python programming expert. Your job is to review the python code using AWS SDK for pandas (awswrangler) to make sure the code is correct.\n",
    "Let's review line by line. Think step by step. \n",
    "\n",
    "{question}\n",
    "\n",
    "Using the context to double check if all the APIs exist in the context\n",
    "{context}\n",
    "\n",
    "\n",
    "Begin!\n",
    "Task: <the orignal task required>\n",
    "Code: <the original code>\n",
    "Findings: <list all findings from the code review>\n",
    "\n",
    "'''\n",
    "\n",
    "step2_code_review_prompt= PromptTemplate(template=step2_code_review_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs = {\"prompt\": step2_code_review_prompt}\n",
    "step2_code_review_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "# step2_code_review_chain = LLMChain(prompt=step2_code_review_prompt, llm=llm)\n",
    "\n",
    "\n",
    "step3_code_improve_template = '''You are a python programming expert. Your job is to rewrite the python code using AWS SDK for pandas (awswrangler) based on the findings.\n",
    "Let's think step by step. \n",
    "\n",
    "Findings: {step2}\n",
    "\n",
    "Only output the python with comments!'''\n",
    "\n",
    "step3_code_improve_prompt= PromptTemplate(template=step3_code_improve_template, input_variables=[\"step2\"])\n",
    "# chain_type_kwargs = {\"prompt\": step2_code_review_prompt}\n",
    "# step2_code_review_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "step3_code_improve_chain = LLMChain(prompt=step3_code_improve_prompt, llm=llm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[step1_code_plan_chain, step2_code_gen_chain, step2_code_review_chain, step3_code_improve_chain], verbose=True)\n",
    "\n",
    "code_gen = overall_chain.run(\"Load data from s3 into Redshift\")\n",
    "\n",
    "print(code_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[step1_gen_code_chain, step2_code_review_chain, step3_code_improve_chain], verbose=True)\n",
    "\n",
    "code_gen = overall_chain.run(\"Query awswrangler inde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
