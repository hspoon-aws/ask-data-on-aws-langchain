{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Demo:\n",
    " A demo that load the S3 bucket PDF into Vector Store (Opensearch) with Bedrock Embdedding. Then QnA using LLM with RetrievalQA chain provided by LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RAG\n",
    "- Load Document (PDF, docx, text) from S3 \n",
    "- Store into Vector store \n",
    "- QnA using LLM with RetrievalQA chain provided by LangChain\n",
    "\n",
    "Ref: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -qqq install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\" \\\n",
    "    \"sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain==0.0.309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set True to enable debug mode\n",
    "import langchain\n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "AWS_REGION = 'us-east-1'\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION']=AWS_REGION\n",
    "os.environ['AWS_ACCESS_KEY_ID']=''\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=''\n",
    "os.environ['AWS_SESSION_TOKEN']=''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init LLM model (Llama-2 70b chat on Amazon Sagemaker JumpStart )\n",
    "In this demo, we use Llama-2 70b chat model as LLM Foundation model, hosted by Amazon Sagemaker JumpStart Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Sagemaker endpoint model name here\n",
    "SAGEMAKER_LLM_MODEL_NAME = 'jumpstart-dft-meta-textgeneration-llama-2-70b-f'\n",
    "SAGEMAKER_IAM_ROLE_NAME = 'AmazonSageMaker-ExecutionRole' # please set IAM role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy Sagemaker JumpStart (Please make sure the EC2 service quota is set)\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import boto3\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName=SAGEMAKER_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "model = JumpStartModel(model_id='meta-textgeneration-llama-2-70b-f',role=role)\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    endpoint_name=SAGEMAKER_LLM_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "def init_llm_sm_endpoint():\n",
    "\n",
    "    endpoint_name = SAGEMAKER_LLM_MODEL_NAME\n",
    "    aws_region=AWS_REGION\n",
    "    parameters = {\"max_new_tokens\": 1000, \"temperature\": 0.1}\n",
    "\n",
    "    class ContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        # LLAMA-2 chat\n",
    "        def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "            input_str = json.dumps({\"inputs\" : [[{\"role\" : \"system\",\n",
    "            \"content\" : \"You are QnA bot to answer the questions based on the context. If it is not in the context, just reply you don't know.\"},\n",
    "            {\"role\" : \"user\", \"content\" : prompt}]],\n",
    "            \"parameters\" : {**model_kwargs}})\n",
    "            return input_str.encode('utf-8')\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            return response_json[0][\"generation\"][\"content\"]\n",
    "        \n",
    "        # Flan-T5\n",
    "        # def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        #     input_str = json.dumps({\"text_inputs\" : prompt, **model_kwargs})\n",
    "        #     return input_str.encode('utf-8')\n",
    "\n",
    "        # def transform_output(self, output: bytes) -> str:\n",
    "        #     response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #     return response_json['generated_texts'][0]\n",
    "\n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    sm_llm = SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_handler,\n",
    "        endpoint_kwargs={\"CustomAttributes\": \"accept_eula=true\"},\n",
    "    )\n",
    "    return sm_llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test for LLM\n",
    "llm = init_llm_sm_endpoint()\n",
    "text = \"What would be a good AWS new service name that allow customers to chat with their own data? Keep answer short. Just give me one answer in your reponse\"\n",
    "llm(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding (Amazon Bedrock Embeddings)\n",
    "We use Amazon Bedrock Titan Embedding as Embedding Foundation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import boto3\n",
    "\n",
    "def init_eb_bedrock():\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='Bedrock-role')['Role']['Arn'] # please set IAM role\n",
    "\n",
    "    sts_client = boto3.client('sts')\n",
    "\n",
    "    assumed_role_object=sts_client.assume_role(\n",
    "        RoleArn=role,\n",
    "        RoleSessionName=\"AssumeRoleSession1\"\n",
    "    )\n",
    "    credentials=assumed_role_object['Credentials']\n",
    "\n",
    "    bedrock_client = boto3.client('bedrock-runtime',\n",
    "                                region_name=AWS_REGION,\n",
    "                                aws_access_key_id=credentials[\"AccessKeyId\"],\n",
    "                                aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
    "                                aws_session_token=credentials[\"SessionToken\"])\n",
    "\n",
    "\n",
    "    # modelId = \"amazon.titan-embed-g1-text-02\"\n",
    "    modelId = \"amazon.titan-embed-text-v1\"\n",
    "    bedrock_embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_client,\n",
    "        region_name=AWS_REGION,\n",
    "        model_id=modelId, \n",
    "    )\n",
    "\n",
    "    return bedrock_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test for embedding\n",
    "bedrock_embeddings = init_eb_bedrock()\n",
    "print(bedrock_embeddings.model_id)\n",
    "embedding_vectors = bedrock_embeddings.embed_documents(['hello', 'world'])\n",
    "print(\"len(embedding_vectors): \", len(embedding_vectors))\n",
    "print(\"sample vector:\\n\",embedding_vectors[0][0:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Store (Amazon Opensearch service)\n",
    "We used Amazon Opensearch service as the vector store. Tried to load S3 bucket documents (AWS Well-Arhitected Framework whitepapers) into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME = '<your_name>-genai-data'\n",
    "S3_BUCKET_KEY = 'wa_whitepapers'\n",
    "\n",
    "OPENSEARCH_URL = \"https://search-vectorstore-<your_openserch>.us-east-1.es.amazonaws.com\"\n",
    "OPENSEARCH_VECTOR_INDEX = \"rag-demo-aws-wa-whitepapers\"\n",
    "\n",
    "\n",
    "# get secret for OPENSEARCH_http_auth {\"username\": \"xxx\", \"password\": \"xxx\"}\n",
    "secret_name = \"vectorstore/opensearch/secret\" \n",
    "\n",
    "session = boto3.session.Session()\n",
    "client = session.client(\n",
    "    service_name='secretsmanager',\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "get_secret_value_response = client.get_secret_value(\n",
    "    SecretId=secret_name\n",
    ")\n",
    "secret = json.loads(get_secret_value_response['SecretString'])\n",
    "OPENSEARCH_http_auth=(secret['username'], secret['password']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 prepare sample pdf (AWS Well architected whitepapers) and upload to s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ./wa_whitepapers\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/framework/wellarchitected-framework.pdf\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/reliability-pillar/wellarchitected-reliability-pillar.pdf\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/operational-excellence-pillar/wellarchitected-operational-excellence-pillar.pdf\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/performance-efficiency-pillar/wellarchitected-performance-efficiency-pillar.pdf\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/cost-optimization-pillar/wellarchitected-cost-optimization-pillar.pdf\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/security-pillar/wellarchitected-security-pillar.pdf\n",
    "! curl -O --output-dir ./wa_whitepapers https://docs.aws.amazon.com/pdfs/wellarchitected/latest/sustainability-pillar/wellarchitected-sustainability-pillar.pdf\n",
    "\n",
    "! aws s3 cp  ./wa_whitepapers s3://<'<your_name>-genai-data'>/wa_whitepapers/ --recursive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load content from S3 Directory. Split into churns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import S3DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "loader = S3DirectoryLoader(S3_BUCKET_NAME, prefix=S3_BUCKET_KEY)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "\n",
    "all_splits = loader.load_and_split(text_splitter)\n",
    "print(f\"Original: Number of document splits = {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sample split:\\n\", all_splits[0:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Embedding and Store into Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use AOS as vector store\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "\n",
    "service = 'es' # must set the service as 'aoss' for Amazon OpenSearch Serverless\n",
    "region = AWS_REGION\n",
    "\n",
    "\n",
    "## for AWSAuth login ##\n",
    "# import boto3\n",
    "# credentials = boto3.Session(aws_access_key_id='xxxxxx',aws_secret_access_key='xxxxx').get_credentials()\n",
    "# awsauth = AWS4Auth('xxxxx', 'xxxxxx', region,service, session_token=credentials.token)\n",
    "\n",
    "vectorstore = OpenSearchVectorSearch.from_documents(\n",
    "    all_splits,\n",
    "    init_eb_bedrock(), # embedding model\n",
    "    opensearch_url=OPENSEARCH_URL,\n",
    "    http_auth=OPENSEARCH_http_auth,\n",
    "    timeout = 600,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=OPENSEARCH_VECTOR_INDEX,\n",
    "    engine=\"faiss\",\n",
    "    bulk_size=10000\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "vectorstore = OpenSearchVectorSearch(\n",
    "    opensearch_url=OPENSEARCH_URL,\n",
    "    index_name=OPENSEARCH_VECTOR_INDEX,\n",
    "    embedding_function=init_eb_bedrock(),\n",
    "    http_auth=OPENSEARCH_http_auth,\n",
    "    timeout = 600,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    engine=\"faiss\"\n",
    ")\n",
    "\n",
    "\n",
    "question = \"What is AWS Well Architected Framework?\"\n",
    "# question='Stop guessing your capacity needs in General design principle'\n",
    "docs = vectorstore.similarity_search(question, k=20)\n",
    "print(f\"Vector search: Number of document related to the question = {len(docs)}\")\n",
    "print('sample result:\\n', docs[1])\n",
    "print('sample result:\\n', docs[3])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QnA the content using RetrievalQA chain\n",
    "QA using a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def find_metadata_sources_from_documents(documents):\n",
    "    source_list = []\n",
    "    for document in documents:\n",
    "        if 'source' in document.metadata:\n",
    "            source = document.metadata['source']\n",
    "            source_list.append(source)\n",
    "    #dedup\n",
    "    source_list = list(dict.fromkeys(source_list))\n",
    "    return source_list\n",
    "\n",
    "llm = init_llm_sm_endpoint()\n",
    "# retreiver = vectorstore.as_retriever(search_kwargs={'k': 20})\n",
    "retreiver = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={'k': 6, 'score_threshold': 0.8})\n",
    "# retreiver = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6, 'lambda_mult': 0.25})\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    #    chain_type=\"stuff\", #refine, map_reduce\n",
    "                                       retriever=retreiver,\n",
    "                                       return_source_documents=True)\n",
    "\n",
    "#unit test\n",
    "question = \"What is AWS Well Architected Framework? summarize in 100 words\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"answer:\\n\", result['result'])\n",
    "print(\"\\nsource:\\n\", find_metadata_sources_from_documents(result['source_documents']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Stop guessing your capacity needs in General design principle? explain to a 5-year-old kid\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"answer:\\n\", result['result'])\n",
    "print(\"\\nsource:\\n\", find_metadata_sources_from_documents(result['source_documents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question that are not related to the documents\n",
    "question = \"Why Siu mei is the best Dim sum in Hong Kong?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"answer:\\n\", result['result'])\n",
    "print(\"\\nsource:\\n\", find_metadata_sources_from_documents(result['source_documents']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Think step by step. What is the multi-AZ?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"answer:\\n\", result['result'])\n",
    "print(\"\\nsource:\\n\", find_metadata_sources_from_documents(result['source_documents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
